import os
import warnings
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate, FewShotPromptTemplate
from langchain.chains import RetrievalQA
from langchain_text_splitters import RecursiveCharacterTextSplitter

# UyarÄ±larÄ± kapat
warnings.filterwarnings("ignore", category=DeprecationWarning)

# OpenAI API anahtarÄ±nÄ± ayarla (Kendi anahtarÄ±nÄ± eklemelisin)
os.environ["OPENAI_API_KEY"] = "enter_api_key"

# PDF'den Anayasa metnini yÃ¼kle
loader = PyPDFLoader("anayasa_eng.pdf")
documents = loader.load()

# Metni kÃ¼Ã§Ã¼k parÃ§alara bÃ¶l
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(documents)

# Chroma vektÃ¶r veritabanÄ±nÄ± oluÅŸtur
vectorstore = Chroma.from_documents(splits, OpenAIEmbeddings())

# Retriever oluÅŸtur
retriever = vectorstore.as_retriever()

# AÃ§Ä±k kaynaklÄ± LLM (Mistral) kullanÄ±mÄ±
llm = Ollama(model="mistral")

### --- Zero-Shot Prompting ---
zero_shot_prompt = PromptTemplate(
    input_variables=["question"],
    template="Soru: {question}\nCevap:"
)

### --- Few-Shot Prompting ---
# Ã–rnek soru-cevaplar oluÅŸtur
examples = [
    {"question": "TÃ¼rkiye Cumhuriyeti'nin resmi dili nedir?", "answer": "TÃ¼rkiye Cumhuriyeti'nin resmi dili TÃ¼rkÃ§edir."},
    {"question": "CumhurbaÅŸkanÄ± kaÃ§ yÄ±l gÃ¶rev yapar?", "answer": "CumhurbaÅŸkanÄ± 7 yÄ±l sÃ¼reyle gÃ¶rev yapar."},
    {"question": "TÃ¼rkiye'nin baÅŸkenti neresidir?", "answer": "TÃ¼rkiye'nin baÅŸkenti Ankara'dÄ±r."}
]

# Few-Shot Prompt Template
example_template = """
Soru: {question}
Cevap: {answer}
"""
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=PromptTemplate(input_variables=["question", "answer"], template=example_template),
    suffix="Soru: {question}\nCevap:",
    input_variables=["question"]
)

# KullanÄ±cÄ±dan aldÄ±ÄŸÄ± inputa gÃ¶re uygun prompting tekniÄŸini belirleyen fonksiyon
def get_prompting_type(question, few_shot=True):
    if few_shot:
        return few_shot_prompt.format(question=question)
    else:
        return zero_shot_prompt.format(question=question)

# RetrievalQA zinciri oluÅŸtur
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever
)

# Sohbet geÃ§miÅŸi iÃ§in Streamlit session state
if "history" not in st.session_state:
    st.session_state.history = []

# KullanÄ±cÄ± ve model mesajlarÄ±nÄ± saklama fonksiyonu
def add_to_history(user_message, bot_response):
    st.session_state.history.append({"role": "user", "content": user_message})
    st.session_state.history.append({"role": "assistant", "content": bot_response})

# GeÃ§miÅŸ sohbeti almak iÃ§in fonksiyon
def get_conversation_context():
    return [{"role": msg["role"], "content": msg["content"]} for msg in st.session_state.history]

# LLM ile sohbet fonksiyonu
def chat_with_llm(user_input, few_shot=True):
    # Zero-shot veya Few-shot prompting'i seÃ§
    prompt = get_prompting_type(user_input, few_shot=few_shot)

    # LLM'den yanÄ±t al
    response = qa_chain.invoke(prompt)

    # GeÃ§miÅŸi gÃ¼ncelle
    add_to_history(user_input, response["result"])

    return response["result"]

# ---- Streamlit UI ----
st.title("Turkish Constitution Chatbot ğŸ‡¹ğŸ‡·ğŸ“œ")

# KullanÄ±cÄ±dan Few-Shot mÄ±, Zero-Shot mÄ± kullanacaÄŸÄ±nÄ± seÃ§mesini iste
few_shot_toggle = st.checkbox("Few-Shot Prompting Kullan", value=True)

# Sohbet geÃ§miÅŸini gÃ¶ster
for message in st.session_state.history:
    role = "You" if message["role"] == "user" else "Bot"
    st.text_area(role, value=message["content"], height=75, disabled=True)

# KullanÄ±cÄ± giriÅŸi
user_input = st.text_input("You: ", key="input")

# GÃ¶nder butonu
if st.button("Send"):
    if user_input.lower() == "exit":
        st.stop()
    else:
        response = chat_with_llm(user_input, few_shot=few_shot_toggle)
        st.rerun()  
